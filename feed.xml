<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://conor-13.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://conor-13.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-28T00:56:47+00:00</updated><id>https://conor-13.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Neural Network Training and Life Progress</title><link href="https://conor-13.github.io/blog/2025/neural-network-training-and-life-progress/" rel="alternate" type="text/html" title="Neural Network Training and Life Progress"/><published>2025-06-25T06:30:00+00:00</published><updated>2025-06-25T06:30:00+00:00</updated><id>https://conor-13.github.io/blog/2025/neural-network-training-and-life-progress</id><content type="html" xml:base="https://conor-13.github.io/blog/2025/neural-network-training-and-life-progress/"><![CDATA[<div align="center" style="font-weight: bold">Introduction</div> <p>Throughout the final year of my undergraduate degree at Princeton, I spent a lot of time thinking about how I could best approach the achievement of my goals and the realization of the evolving vision I devised for my life. I also spent a lot of time studying and training neural networks for my thesis. These threads of attention and cognitive effort largely felt orthogonal to one another: the former was strictly a personal and humanistic matter and the latter an impersonal and technical one. I compartmentalized the topics accordingly and never deliberately tried to connect them.</p> <p>Gradually and organically, however, I began to appreciate – or manufacture – their parallelism. The problem of transiting from my current state in life to my desired state in life, I realized, may not be dissimilar from the problem of transiting from a randomly-initialized neural network to one that is maximally performant at a given predictive or generative task. I became interested in translating the intuition I developed around how neural networks iteratively acquire their impressive capabilities into a mental framework for understanding and advancing my own journey.</p> <p>In this blog post, I present the product of these translational reflections. I first introduce and briefly explain the essence of gradient descent – the optimization algorithm through which neural networks learn. I then make the case that gradient descent is a powerful model for the process of making progress in one’s life. I hope the ideas I relay here are as useful to you as they have felt to me.</p> <div align="center" style="font-weight: bold">The Essence of Gradient Descent</div> <p>To explain the essence of gradient descent, I’ll start by constructing a model that represents the process of neural network training as simple interactions between simple computational objects.</p> <p>Suppose there is a list of numerical inputs <em>[x]</em> and a list of numerical outputs <em>[y]</em>. Suppose further that a function <em>f(x)</em> maps each numerical input in <em>[x]</em> to a numerical output in <em>[y]</em>. The objective is to train a neural network to predict <em>y</em> based on <em>x</em>, which essentially requires that the neural network learn <em>f(x)</em>.</p> <p>Accordingly, the neural network can be represented as <em>f’(x)</em> – an approximation of <em>f(x)</em>. Upon initialization, <em>f’(x)</em> is a random function that likely does not resemble <em>f(x)</em> at all. The instrument that is employed to change this – to minimize the difference between <em>f’(x)</em> and <em>f(x)</em> – is the gradient descent algorithm. Here’s how it works:</p> <ol> <li>Each element of <em>[x]</em> is passed through <em>f’(x)</em>, generating a prediction <em>y’</em>.</li> <li>The error between the predicted output <em>y’</em> and the true output <em>y</em> is calculated, as defined by a loss function.</li> <li>The derivative – the “gradient” – of the loss function with respect to the parameters that define <em>f’(x)</em> is calculated; this quantifies how directional alterations in <em>f’(x)</em> make <em>f’(x)</em> more similar or less similar to <em>f(x)</em>.</li> <li><em>f’(x)</em> is updated in the direction that reduces the value of the loss function and thus makes <em>f’(x)</em> more similar to <em>f(x)</em>. The magnitude of the update is governed by the value of the learning rate hyperparameter defined at the onset of training.</li> </ol> <p>These steps are repeated over several iterations until convergence, at which point <em>f’(x)</em> should closely resemble <em>f(x)</em>.</p> <p>This is gradient descent in the context of neural network training. Qualitatively, it is a procedure for minimizing the difference between the initial functionality of a neural network and the desired functionality of a neural network. I’ll reinforce these concepts in the next section as I adapt them into a framework for life progress.</p> <div align="center" style="font-weight: bold">Gradient Descent as a Life Framework</div> <p>I’ll make the case for gradient descent as a life framework by exchanging the mathematical objects on which the algorithm natively operates with the personal objects of progress; I’ll use the model I constructed in the previous section to explain this.</p> <p>Just as the objective of neural network training is to achieve a desired functionality within a neural network starting from a simple initialization, I posit that the objective of human progress is to reach a desired state in life from an initial state in life.</p> <p>Accomplishing this objective through one perfect, calculated action feels – and arguably is – infeasible, as is training an optimal neural network in one shot. Instead, a reasonable approach to progressing towards a desired state involves first taking an action; it involves first doing <em>something</em>, just as neural network training begins by randomly intializing the parameters of the network.</p> <p>The next step is to assess how far you are from accomplishing your goal after taking that action – to calculate the error between the result of your action and not your expected result but your North Star.</p> <p>What follows is analogous to the process of calculating the derivative of the loss function and updating the parameters of the network. You should determine which direction for your next action feels most aligned with your desired state and decide how risky an action you wish to take. The magnitude of risk is captured in the learning rate hyperparameter in neural network training.</p> <p>The process of life progress, I believe, is thus no different than vanilla gradient descent.</p>]]></content><author><name></name></author><category term="AI,"/><category term="Entrepreneurship"/><summary type="html"><![CDATA[I identify the parallelism between how neural networks learn and how individuals make progress in their lives.]]></summary></entry><entry><title type="html">Uncertainty and Risk in Machine Learning and Life</title><link href="https://conor-13.github.io/blog/2025/risk-in-machine-learning-and-life/" rel="alternate" type="text/html" title="Uncertainty and Risk in Machine Learning and Life"/><published>2025-06-25T06:30:00+00:00</published><updated>2025-06-25T06:30:00+00:00</updated><id>https://conor-13.github.io/blog/2025/risk-in-machine-learning-and-life</id><content type="html" xml:base="https://conor-13.github.io/blog/2025/risk-in-machine-learning-and-life/"><![CDATA[<div align="center" style="font-weight: bold">Introduction</div> <p>Until recently, the explanatory power of the risk-reward framework was lost on me. The ubiquity of advice that encourages risk-taking and the abundance of quotes that variate on the theme of “no risk, no reward” familiarized me with the concept of risk to such a degree that the topic did not feel novel enough or controversial enough to think about. I subconsciously trusted that such advice and quotes were <em>probably</em> true – that, indeed, an individual’s outcome in life could be attributed at least in part to the magnitudes and directions of the risks that the individual took. But I understood neither how nor why this was the case.</p> <p>While realizing and developing the analogy between neural network training and life progress that I wrote about in my last blog post, I analyzed more closely the interaction between risk and achievement in machine learning systems and humans systems alike. The result of such analysis was the deeper appreciation for the necessity of risk-taking in the search for optimal machine learning models and the pursuit of personal fulfilment that I will describe throughout the rest of this post.</p> <p>Here, I first elaborate on the claim that the learning rate hyperparameter is an embodiment of risk. I examine the effect of small, moderate, and large learning rates on the performance trajectory of machine learning models during training. I then return to the human domain, justifying on top of the prior context how and why, in my view, risk is a necessary ingredient for the fulfilment of personal ambitions.</p> <div align="center" style="font-weight: bold">Learning Rate as a Measure and Modulator of Risk</div> <p>In the context of neural network training, the learning rate hyperparameter governs the extent to which the parameters of a neural network are updated or changed during training. When the learning rate is very small, the parameters of the network change very slightly; when the learning rate is very large, the parameters of the network change very significantly; and when the learning rate is zero, the parameters of the network do not change at all. This maps well onto the commonsense understanding of the relationship between risk and reward in life: small risks generally yield small changes, large risks generally yield large changes, and the absense of risk altogether generally yields no change at all.</p> <p>The analogy becomes more complete when we acknowledge how parameter changes of different magnitudes, as induced by learning rates of different magnitudes, tend to affect the convergence behavior of machine learning systems during training:</p> <ul> <li>Very slight changes in the parameters of a machine learning model during training tend to move the state of the model towards the point of optimality, but the movement is not by much; it is likely that the model, just narrowly inching in the right direction, will never reach the point of optimality in a reasonable timeframe under these conditions.</li> <li>Very significant changes in the parameters of a machine learning model during training tend to move the state of the model significantly, but the movement may overshoot the point of optimality by a large margin; it is likely that the model, volleying and long jumping between distant suboptimal states that straddle the point of optimality, will never reach the point of optimality under these conditions.</li> <li>The absense of change in the parameters of a machine learning model during training, of course, will not move the state of the model at all; unless it was luckily initialized there, the model will never reach the point of optimiality under these conditions.</li> </ul> <p>The assertion that a very small learning rate generally yields a model that never reaches the point of optimality while a model trained with a very large learning rate overshoots it suggests the existence of a moderate learning rate that achieves what the extremes do not. To me, this solidifies learning rate as a representation of risk magnitude: the relationship between risk magnitude and expected life outcome seems to parallel that betweeen learning rate and expected model quality, as I discuss in the next section.</p> <div align="center" style="font-weight: bold">Risk as a Necessary Ingredient in the Pursuit of Personal Fulfilment</div> <p>My understanding of risk as a necessary ingredient in the pursuit of personal fulfilment and the realization of one’s ambitions aligns with this analogy. When an individual iteratively takes very small risks, they make progress towards the ideal outcomes to which they aspire, but the progress may be too slow and incremental for the individual to ever achieve those outcomes. When an individual iteratively takes very large risks, they may overshoot the ideal the ideal outcomes to which they aspire in a favorable or unfavorable direction, the flip-flopping whiplash of which may prevent them reaching the desired state. And when an individual takes no risks, they stagnate and move no nearer to or further from the realization of their goals. It is when an individual iteratively takes risks of a moderate and appropriate size that they make sufficient progress and build sufficient momentum to satisfy the objective.</p> <p>To me, this lesson seems to explain the classical and archetypal stories of great accomplishment and impact in entrepreneurial ventures spanning technology, academia, and medicine; I’ll save a presentation and discussion of such stories for a future post.</p> <p>In conclusion, we can rationalize and build an intuition for the risk-reward framework in life by examining its influence in the context of machine learning. We observe that risk-taking seems vital to the achievement of an optimal outcome and that risk-sizing is an essential exercise here.</p> <p>Note: I do not attempt here to define or provide specific examples of very small risks, very large risks, the absense of risk, or perfectly-sized risks. The magnitude of a risk is, as I see it, extremely circumstantial, varying as a function of an infinite set of variables that includes much more than even an individual’s history and current state . To craft a scenario that effectively relays, appropriately weighs, and captures with nuance the interations between all these variables in sizing a risk is infeasible.</p>]]></content><author><name></name></author><category term="AI,"/><category term="Entrepreneurship"/><summary type="html"><![CDATA[I discuss risk as a necessary input for high achievement in machine learning systems and human systems alike.]]></summary></entry><entry><title type="html">Decision Trees and Human Collaboration</title><link href="https://conor-13.github.io/blog/2025/decision-trees-and-human-collaboration-copy/" rel="alternate" type="text/html" title="Decision Trees and Human Collaboration"/><published>2025-01-15T12:32:00+00:00</published><updated>2025-01-15T12:32:00+00:00</updated><id>https://conor-13.github.io/blog/2025/decision-trees-and-human-collaboration%20copy</id><content type="html" xml:base="https://conor-13.github.io/blog/2025/decision-trees-and-human-collaboration-copy/"><![CDATA[<p>My coursework sometimes piques my curiosity in ways likely unintended. When I learn a new concept in class, for example, it sometimes interests me not academically but translationally. I become intrigued by how the concept offers wisdom to domains totally outside that from which it came, and I find it satisfying to think through these incidental connections, even if they are contrived, elementary, and imprecise.</p> <p>This experience was surprisingly frequent this semester. While taking courses on machine learning, organic chemistry, ecology, and evolutionary biology, I came upon several translational lessons I believe are worth sharing here. I proceed here by describing from a machine learning course the first of these lessons and the concepts that underlie it. I describe what we can learn about human collaboration by studying decision tree ensembles.</p> <div align="center" style="font-weight: bold">Decision Tree Ensembles -- How to Make Good Decisions in Collaboration</div> <p><br/> The decision tree architecture learns to predict a target feature value by determining hierachically which compositions of descriptive feature values correspond to which target feature values in a training dataset. A single decision tree by itself can model complex relationships, but several such decision trees can be ensembled into an aggregate model with a greater representational capacity.</p> <p>In <em>Fundamentals of Machine Learning for Predictive Data Analytics</em>, authors Kelleher et al. state a heuristic condition on the effectiveness of ensembling: for an ensemble to be more performant than one of its members, each member of the ensemble should be distinct. The intuition behind this condition is fairly straightforward: if each member of the ensemble is not sufficiently distinct, then the ensemble’s inferences could be generated by just one of its members (i.e., an ensemble would possess no significant advantage over any one of its members). In this sense, an ensemble with significant redundancy across its members engages in “group think,” amplifying the homogeneous biases of its membership.</p> <p>Kelleher et al. briefly mention that this principle applies equally to ensembles of humans - a suggestion that has captivated me since I first read it. As I interpret it, this comment lends itself to a theory of collaboration which implies that, in general, strong and productive human organizations will consist of individuals that (1) approach problems differently, (2) are independently competent, and (3) are tuned to the same objective. As I have realized, it is the synergy between these qualities - not the isolated merit of each individual quality - that is powerful. Why each of these qualities alone is insufficient and why each pair of these qualities is weaker without the third can be understood through the following analysis. The effectiveness of an organization is improbable (or the collaboration it houses is unjustifiable) when:</p> <ul> <li>Its members approach problems differently but each approach leads to poor results (i.e., members are not independently competent) or each approach achieves conflicting ends (i.e., members are not tuned to the same objecive).</li> <li>Its members are independently competent but are redundant in their methodology (i.e., members do not approach problems differently) or possess radically different ambitions (i.e., members are not tuned to the same objective).</li> <li>Its members are tuned to the same objective but pursue that objective in identical ways (i.e., members do not approach problems differently) or pursue that objective ineffectively (i.e., members are not independently competent).</li> </ul> <p>Note: The independent competence argument can be overturned if the absence of independent competence is accompanied by collaborative competence, though it is arguable that independent competence is necessary for the latter. Regardless, this is an edge case I will ignore here.</p> <p>I find this connection satisfying and translational because the basic criteria for an effective ensemble of decision trees can be extended into a set of criteria that retroactively help to explain why the “good” teams I’ve been on were “good” and why the “bad” teams I’ve been on were “bad.” The way I now understand and approach organized collaboration has tranformed due to these insights.</p>]]></content><author><name></name></author><category term="AI,"/><category term="Entrepreneurship"/><summary type="html"><![CDATA[I describe what we can learn about human collaboration by studying decision tree ensembles.]]></summary></entry></feed>